{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf01c8c-2fb5-4273-911f-390b55da2f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Warning: JSAnimation not found\n",
      "*** Warning: JSAnimation not found\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from exact_solvers import euler\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim, nn, utils, Tensor\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "import mlflow.pytorch\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1462e815-9d03-4386-8ebd-d6ea5cb8e95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "State = euler.Primitive_State\n",
    "scaler = StandardScaler()\n",
    "\n",
    "stencil = 5\n",
    "offset = stencil//2\n",
    "\n",
    "inputs = []\n",
    "labels = []\n",
    "\n",
    "gammas = np.linspace(1.1, 2.0, 10)\n",
    "times = np.linspace(1e-10, 1.0, 100)\n",
    "PLs = np.linspace(3, 13, 5)\n",
    "# PRs = np.linspace(3, 13, 5)\n",
    "PRs = [0.1]\n",
    "# grid_sizes = np.power(2, np.arange(6, 12))\n",
    "grid_sizes = [512]\n",
    "\n",
    "for gs in grid_sizes:\n",
    "    print(gs)\n",
    "    N = gs\n",
    "    a = -2.0\n",
    "    b = 2.0\n",
    "\n",
    "    x = np.linspace(a, b, N)\n",
    "    dx = (b - a)/N\n",
    "    \n",
    "    div = np.zeros(N)\n",
    "    \n",
    "    for PL in PLs:\n",
    "        for PR in PRs:\n",
    "            left_state  = State(Density = 3.,\n",
    "                                Velocity = 0.,\n",
    "                                Pressure = PL)\n",
    "            right_state = State(Density = 1.,\n",
    "                                Velocity = 0.,\n",
    "                                Pressure = PR)\n",
    "            \n",
    "            q_l = euler.primitive_to_conservative(*left_state)\n",
    "            q_r = euler.primitive_to_conservative(*right_state)\n",
    "            \n",
    "            \n",
    "            for gamma in gammas:\n",
    "            \n",
    "                states, speeds, reval, wave_types = euler.exact_riemann_solution(q_l, q_r, gamma=gamma)    \n",
    "                \n",
    "                for t in times:\n",
    "                    rho, u, P = euler.cons_to_prim(reval(x/t))\n",
    "\n",
    "                    if (PL > PR):\n",
    "                        \n",
    "                        raref_l, raref_r = speeds[0][0] * t, speeds[0][1] * t\n",
    "                        contact = speeds[1] * t\n",
    "                        shock = speeds[2] * t\n",
    "                    elif (PR > PL):\n",
    "                        raref_l, raref_r = speeds[2][0] * t, speeds[2][1] * t\n",
    "                        contact = speeds[1] * t\n",
    "                        shock = speeds[0] * t\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                    div[1:-2] = (u[2:-1] - u[0:-3])/(2 * dx)\n",
    "                    rho, u, P, div = StandardScaler().fit_transform(rho.reshape(-1,1)).flatten(), StandardScaler().fit_transform(u.reshape(-1,1)).flatten(), StandardScaler().fit_transform(P.reshape(-1,1)).flatten(),StandardScaler().fit_transform(div.reshape(-1,1)).flatten()\n",
    "                    \n",
    "                    #shock_loc = np.logical_and(x > (shock - eps),x < (shock + eps))\n",
    "                    for i in range(offset, N - offset):\n",
    "                        input = np.zeros((2,stencil))\n",
    "                        input[0, :] = rho[(i-offset):(i + offset + 1)]\n",
    "                        #input[0, :] -= np.mean(rho[(i-offset):(i + offset + 1)])\n",
    "                        # input[1, :] = u[(i-offset):(i + offset + 1)]\n",
    "                        #input[1, :] -= np.mean(u[(i-offset):(i + offset + 1)])\n",
    "                        input[1, :] = P[(i-offset):(i + offset + 1)]\n",
    "                        #input[2, :] -= np.mean(P[(i-offset):(i + offset + 1)])\n",
    "                        # input[3, :] = div[(i-offset):(i + offset + 1)]\n",
    "                        #input[3, :] -= np.mean(div[(i-offset):(i + offset + 1)])\n",
    "                        inputs.append(input)\n",
    "                        #if np.any(x[(i-offset):(i + offset + 1)] >= shock) and np.any(x[(i-offset):(i + offset + 1)] <= shock):\n",
    "                        if np.any(x[(i-offset):(i + offset + 1)] > shock) and np.any(x[(i-offset):(i + offset + 1)] < shock):\n",
    "                            labels.append(1)\n",
    "                        else:\n",
    "                            labels.append(0)\n",
    "            \n",
    "\n",
    "# random.seed(42)\n",
    "# indices_0 = [i for i, label in enumerate(labels) if label == 0]\n",
    "# indices_1 = [i for i, label in enumerate(labels) if label == 1]\n",
    "\n",
    "# # Randomly select a subset of 0s with the same size as the number of 1s\n",
    "# subset_indices_0 = random.sample(indices_0, len(indices_1))\n",
    "\n",
    "# # Create new balanced lists\n",
    "# balanced_inputs = [inputs[i] for i in subset_indices_0 + indices_1]\n",
    "# balanced_labels = [labels[i] for i in subset_indices_0 + indices_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a7a71f9-3d2c-4fac-93a5-544075e63ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gammas = np.linspace(1.1, 2.0, 10)\n",
    "times = np.linspace(1e-10, 1.0, 100)\n",
    "PRs = np.linspace(3, 13, 5)\n",
    "# PRs = np.linspace(3, 13, 5)\n",
    "PLs = [0.1]\n",
    "#grid_sizes = np.power(2, np.arange(6, 12))\n",
    "grid_sizes = [512]\n",
    "\n",
    "for gs in grid_sizes:\n",
    "    print(gs)\n",
    "    N = gs\n",
    "    a = -2.0\n",
    "    b = 2.0\n",
    "\n",
    "    x = np.linspace(a, b, N)\n",
    "    dx = (b - a)/N\n",
    "    \n",
    "    div = np.zeros(N)\n",
    "    \n",
    "    for PL in PLs:\n",
    "        for PR in PRs:\n",
    "            left_state  = State(Density = 1.,\n",
    "                                Velocity = 0.,\n",
    "                                Pressure = PL)\n",
    "            right_state = State(Density = 3.,\n",
    "                                Velocity = 0.,\n",
    "                                Pressure = PR)\n",
    "            \n",
    "            q_l = euler.primitive_to_conservative(*left_state)\n",
    "            q_r = euler.primitive_to_conservative(*right_state)\n",
    "            \n",
    "            \n",
    "            for gamma in gammas:\n",
    "            \n",
    "                states, speeds, reval, wave_types = euler.exact_riemann_solution(q_l, q_r, gamma=gamma)    \n",
    "                \n",
    "                for t in times:\n",
    "                    rho, u, P = euler.cons_to_prim(reval(x/t))\n",
    "\n",
    "                    if (PL > PR):\n",
    "                        \n",
    "                        raref_l, raref_r = speeds[0][0] * t, speeds[0][1] * t\n",
    "                        contact = speeds[1] * t\n",
    "                        shock = speeds[2] * t\n",
    "                    elif (PR > PL):\n",
    "                        raref_l, raref_r = speeds[2][0] * t, speeds[2][1] * t\n",
    "                        contact = speeds[1] * t\n",
    "                        shock = speeds[0] * t\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                    div[1:-2] = (u[2:-1] - u[0:-3])/(2 * dx)\n",
    "                    rho, u, P, div = StandardScaler().fit_transform(rho.reshape(-1,1)).flatten(), StandardScaler().fit_transform(u.reshape(-1,1)).flatten(), StandardScaler().fit_transform(P.reshape(-1,1)).flatten(),StandardScaler().fit_transform(div.reshape(-1,1)).flatten()\n",
    "\n",
    "                    \n",
    "                    #shock_loc = np.logical_and(x > (shock - eps),x < (shock + eps))\n",
    "                    for i in range(offset, N - offset):\n",
    "                        input = np.zeros((2,stencil))\n",
    "                        input[0, :] = rho[(i-offset):(i + offset + 1)]\n",
    "                        #input[0, :] -= np.mean(rho[(i-offset):(i + offset + 1)])\n",
    "                        # input[1, :] = u[(i-offset):(i + offset + 1)]\n",
    "                        #input[1, :] -= np.mean(u[(i-offset):(i + offset + 1)])\n",
    "                        input[1, :] = P[(i-offset):(i + offset + 1)]\n",
    "                        #input[2, :] -= np.mean(P[(i-offset):(i + offset + 1)])\n",
    "                        # input[3, :] = div[(i-offset):(i + offset + 1)]\n",
    "                        #input[3, :] -= np.mean(div[(i-offset):(i + offset + 1)])\n",
    "                        inputs.append(input)\n",
    "                        #if np.any(x[(i-offset):(i + offset + 1)] >= shock) and np.any(x[(i-offset):(i + offset + 1)] <= shock):\n",
    "                        if np.any(x[(i-offset):(i + offset + 1)] > shock) and np.any(x[(i-offset):(i + offset + 1)] < shock):\n",
    "                            labels.append(1)\n",
    "                        else:\n",
    "                            labels.append(0)\n",
    "            \n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "indices_0 = [i for i, label in enumerate(labels) if label == 0]\n",
    "indices_1 = [i for i, label in enumerate(labels) if label == 1]\n",
    "\n",
    "# Randomly select a subset of 0s with the same size as the number of 1s\n",
    "subset_indices_0 = random.sample(indices_0, len(indices_1))\n",
    "\n",
    "# Create new balanced lists\n",
    "balanced_inputs = [inputs[i] for i in subset_indices_0 + indices_1]\n",
    "balanced_labels = [labels[i] for i in subset_indices_0 + indices_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efbe1651-9f31-47b7-a5ed-3e3c721311be",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "for i, l in zip(balanced_inputs, balanced_labels):\n",
    "    train_data.append([torch.tensor(i.flatten(), dtype=torch.float32), torch.tensor(l, dtype=torch.float32)])\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "\n",
    "        # Assuming the first element is the input tensor and the second element is the label tensor\n",
    "        input_tensor = torch.Tensor(sample[0])\n",
    "        label_tensor = torch.Tensor(sample[1])\n",
    "\n",
    "        return input_tensor, label_tensor\n",
    "        \n",
    "dataset = CustomDataset(train_data)\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [0.8, 0.2], generator=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97932eea-1045-4005-a316-07fbb81e175f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91133\n",
      "22783\n"
     ]
    }
   ],
   "source": [
    "torch.save(train_set, \"../../../data/ds-2/train_set.pt\")\n",
    "torch.save(valid_set, \"../../../data/ds-2/valid_set.pt\")\n",
    "print (len(train_set))\n",
    "print (len(valid_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69b14353-fe5e-47aa-8efa-0f4cc75c8bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Shock_Cell_Classifier(pl.LightningModule):\n",
    "    def __init__(self, input_dim, lr = 1e-4, wd = 1.0e-5):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # self.fc1 = nn.Linear(self.hparams.input_dim, 16)\n",
    "        # self.fc2 = nn.Linear(16, 2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.hparams.input_dim, self.hparams.input_dim)\n",
    "        self.fc2 = nn.Linear(self.hparams.input_dim, 2)\n",
    "        # self.fc3 = nn.Linear(8, 2)\n",
    "\n",
    "        self.dropout5 = nn.Dropout(0.50)\n",
    "        self.dropout2 = nn.Dropout(0.20)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        # nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        \n",
    "        #Initialize bias terms to zero\n",
    "        nn.init.constant_(self.fc1.bias, 0)\n",
    "        nn.init.constant_(self.fc2.bias, 0)\n",
    "        # nn.init.constant_(self.fc3.bias, 0)\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        # self.logger.log_hyperparams()\n",
    "    def forward(self, x):\n",
    "        # x = self.dropout2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.dropout5(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = F.leaky_relu(x)\n",
    "        # x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        output = self.forward(inputs)\n",
    "\n",
    "        \n",
    "        loss = self.loss_fn(output, labels.long())\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # this is the validation loop\n",
    "        inputs, labels = batch\n",
    "        \n",
    "        output = self.forward(inputs)\n",
    "        \n",
    "        val_loss = self.loss_fn(output, labels.long())\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.lr, weight_decay=self.hparams.wd)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "# init the classifier\n",
    "classifier = Shock_Cell_Classifier(stencil*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7357d819-1ae0-4bbc-becb-a3eda54bb863",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (mps), used: False\n",
      "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
      "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "/Users/henry/euler-nn/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/henry/euler-nn/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "2024/05/22 15:05:07 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of pytorch. If you encounter errors during autologging, try upgrading / downgrading pytorch to a supported version, or try upgrading MLflow.\n",
      "2024/05/22 15:05:07 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/henry/euler-nn/.venv/lib/python3.11/site-packages/mlflow/pytorch/_lightning_autolog.py:463: UserWarning: Autologging is known to be compatible with pytorch-lightning versions between 1.4.9 and 2.2.0.post0 and may not succeed with packages outside this range.\"\n",
      "\n",
      "  | Name     | Type             | Params\n",
      "----------------------------------------------\n",
      "0 | fc1      | Linear           | 110   \n",
      "1 | fc2      | Linear           | 33    \n",
      "2 | dropout5 | Dropout          | 0     \n",
      "3 | dropout2 | Dropout          | 0     \n",
      "4 | loss_fn  | CrossEntropyLoss | 0     \n",
      "----------------------------------------------\n",
      "143       Trainable params\n",
      "0         Non-trainable params\n",
      "143       Total params\n",
      "0.001     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henry/euler-nn/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "/Users/henry/euler-nn/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc32186f13743f7bfd1c47835c5cc0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be1b7c799c564ea9a855c1ec841cd3a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7a882dbd064938aae66098411ca918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5caf7803bcb45ddb61e37dca5b4b415",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b2528f37554adeab4483b5016c17cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97780b1ee584b258786fcec9c33b759",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90ae8312e6ad422bae8f21435fb2265a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2f16c79c78496ba67822b18a5d7e02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/Users/henry/euler-nn/.venv/lib/python3.11/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run() \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# mlflow.pytorch.log_model(classifier, \"model\")\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_params({\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch_size,\n\u001b[1;32m     16\u001b[0m         })\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:578\u001b[0m, in \u001b[0;36msafe_patch.<locals>.safe_patch_function\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     patch_function\u001b[38;5;241m.\u001b[39mcall(call_original, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 578\u001b[0m     \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    580\u001b[0m session\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msucceeded\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m try_log_autologging_event(\n\u001b[1;32m    583\u001b[0m     AutologgingEventLogger\u001b[38;5;241m.\u001b[39mget_logger()\u001b[38;5;241m.\u001b[39mlog_patch_function_success,\n\u001b[1;32m    584\u001b[0m     session,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     kwargs,\n\u001b[1;32m    589\u001b[0m )\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/utils/autologging_utils/safety.py:251\u001b[0m, in \u001b[0;36mwith_managed_run.<locals>.patch_with_managed_run\u001b[0;34m(original, *args, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     managed_run \u001b[38;5;241m=\u001b[39m create_managed_run()\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mpatch_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mException\u001b[39;00m, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;66;03m# In addition to standard Python exceptions, handle keyboard interrupts to ensure\u001b[39;00m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;66;03m# that runs are terminated if a user prematurely interrupts training execution\u001b[39;00m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;66;03m# (e.g. via sigint / ctrl-c)\u001b[39;00m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m managed_run:\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/pytorch/_lightning_autolog.py:558\u001b[0m, in \u001b[0;36mpatched_fit\u001b[0;34m(original, self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m log_models:\n\u001b[1;32m    555\u001b[0m     registered_model_name \u001b[38;5;241m=\u001b[39m get_autologging_config(\n\u001b[1;32m    556\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mpytorch\u001b[38;5;241m.\u001b[39mFLAVOR_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregistered_model_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     )\n\u001b[0;32m--> 558\u001b[0m     \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    561\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m early_stop_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path:\n\u001b[1;32m    565\u001b[0m         mlflow\u001b[38;5;241m.\u001b[39mlog_artifact(\n\u001b[1;32m    566\u001b[0m             local_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheckpoint_callback\u001b[38;5;241m.\u001b[39mbest_model_path,\n\u001b[1;32m    567\u001b[0m             artifact_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrestored_model_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    568\u001b[0m         )\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/pytorch/__init__.py:301\u001b[0m, in \u001b[0;36mlog_model\u001b[0;34m(pytorch_model, artifact_path, conda_env, code_paths, pickle_module, registered_model_name, signature, input_example, await_registration_for, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mLog a PyTorch model as an MLflow artifact for the current run.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    PyTorch logged models\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    300\u001b[0m pickle_module \u001b[38;5;241m=\u001b[39m pickle_module \u001b[38;5;129;01mor\u001b[39;00m mlflow_pytorch_pickle_module\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43martifact_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflavor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpytorch_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpytorch_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconda_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconda_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcode_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcode_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregistered_model_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregistered_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_example\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_example\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequirements_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequirements_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_files\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_pip_requirements\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/models/model.py:619\u001b[0m, in \u001b[0;36mModel.log\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m     run_id \u001b[38;5;241m=\u001b[39m mlflow\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39mfluent\u001b[38;5;241m.\u001b[39m_get_or_start_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\n\u001b[1;32m    618\u001b[0m mlflow_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(artifact_path\u001b[38;5;241m=\u001b[39martifact_path, run_id\u001b[38;5;241m=\u001b[39mrun_id, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n\u001b[0;32m--> 619\u001b[0m \u001b[43mflavor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlflow_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlflow_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;66;03m# Copy model metadata files to a sub-directory 'metadata',\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;66;03m# For UC sharing use-cases.\u001b[39;00m\n\u001b[1;32m    623\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(local_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/pytorch/__init__.py:555\u001b[0m, in \u001b[0;36msave_model\u001b[0;34m(pytorch_model, path, conda_env, mlflow_model, code_paths, pickle_module, signature, input_example, requirements_file, extra_files, pip_requirements, extra_pip_requirements, metadata, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m     default_reqs \u001b[38;5;241m=\u001b[39m get_default_pip_requirements()\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;66;03m# To ensure `_load_pyfunc` can successfully load the model during the dependency\u001b[39;00m\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;66;03m# inference, `mlflow_model.save` must be called beforehand to save an MLmodel file.\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m     inferred_reqs \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_pip_requirements\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_data_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mFLAVOR_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_reqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m     default_reqs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[38;5;28mset\u001b[39m(inferred_reqs)\u001b[38;5;241m.\u001b[39munion(default_reqs))\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/utils/environment.py:449\u001b[0m, in \u001b[0;36minfer_pip_requirements\u001b[0;34m(model_uri, flavor, fallback)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Infers the pip requirements of the specified model by creating a subprocess and loading\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;124;03mthe model in it to determine which packages are imported.\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    446\u001b[0m \n\u001b[1;32m    447\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_infer_requirements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflavor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m fallback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/utils/requirements_utils.py:421\u001b[0m, in \u001b[0;36m_infer_requirements\u001b[0;34m(model_uri, flavor)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_infer_requirements\u001b[39m(model_uri, flavor):\n\u001b[1;32m    410\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Infers the pip requirements of the specified model by creating a subprocess and loading\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;124;03m    the model in it to determine which packages are imported.\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    419\u001b[0m \n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 421\u001b[0m     \u001b[43m_init_modules_to_packages_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m _PYPI_PACKAGE_INDEX\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _PYPI_PACKAGE_INDEX \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/mlflow/utils/requirements_utils.py:359\u001b[0m, in \u001b[0;36m_init_modules_to_packages_map\u001b[0;34m()\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _MODULES_TO_PACKAGES\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _MODULES_TO_PACKAGES \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[38;5;66;03m# Note `importlib_metada.packages_distributions` only captures packages installed into\u001b[39;00m\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;66;03m# Python’s site-packages directory via tools such as pip:\u001b[39;00m\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;66;03m# https://importlib-metadata.readthedocs.io/en/latest/using.html#using-importlib-metadata\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     _MODULES_TO_PACKAGES \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpackages_distributions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# Add mapping for MLflow extras\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     _MODULES_TO_PACKAGES\u001b[38;5;241m.\u001b[39mupdate(MLFLOW_MODULES_TO_PACKAGES)\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/importlib_metadata/__init__.py:1054\u001b[0m, in \u001b[0;36mpackages_distributions\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1052\u001b[0m pkg_to_dist \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m distributions():\n\u001b[0;32m-> 1054\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pkg \u001b[38;5;129;01min\u001b[39;00m _top_level_declared(dist) \u001b[38;5;129;01mor\u001b[39;00m \u001b[43m_top_level_inferred\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1055\u001b[0m         pkg_to_dist[pkg]\u001b[38;5;241m.\u001b[39mappend(dist\u001b[38;5;241m.\u001b[39mmetadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(pkg_to_dist)\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/importlib_metadata/__init__.py:1097\u001b[0m, in \u001b[0;36m_top_level_inferred\u001b[0;34m(dist)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_top_level_inferred\u001b[39m(dist):\n\u001b[0;32m-> 1097\u001b[0m     opt_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mmap\u001b[39m(_get_toplevel_name, always_iterable(\u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m)))\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimportable_name\u001b[39m(name):\n\u001b[1;32m   1100\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m name\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/importlib_metadata/__init__.py:535\u001b[0m, in \u001b[0;36mDistribution.files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;129m@pass_none\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mskip_missing_files\u001b[39m(package_paths):\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m path: path\u001b[38;5;241m.\u001b[39mlocate()\u001b[38;5;241m.\u001b[39mexists(), package_paths))\n\u001b[0;32m--> 535\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mskip_missing_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmake_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_files_distinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_files_egginfo_installed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_files_egginfo_sources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/importlib_metadata/_functools.py:102\u001b[0m, in \u001b[0;36mpass_none.<locals>.wrapper\u001b[0;34m(param, *args, **kwargs)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(param, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/importlib_metadata/__init__.py:533\u001b[0m, in \u001b[0;36mDistribution.files.<locals>.skip_missing_files\u001b[0;34m(package_paths)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;129m@pass_none\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mskip_missing_files\u001b[39m(package_paths):\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m path: path\u001b[38;5;241m.\u001b[39mlocate()\u001b[38;5;241m.\u001b[39mexists(), package_paths))\n",
      "File \u001b[0;32m~/euler-nn/.venv/lib/python3.11/site-packages/importlib_metadata/__init__.py:533\u001b[0m, in \u001b[0;36mDistribution.files.<locals>.skip_missing_files.<locals>.<lambda>\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;129m@pass_none\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mskip_missing_files\u001b[39m(package_paths):\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mlambda\u001b[39;00m path: \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, package_paths))\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pathlib.py:1235\u001b[0m, in \u001b[0;36mPath.exists\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;124;03mWhether this path exists.\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1235\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _ignore_error(e):\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.7/Frameworks/Python.framework/Versions/3.11/lib/python3.11/pathlib.py:1013\u001b[0m, in \u001b[0;36mPath.stat\u001b[0;34m(self, follow_symlinks)\u001b[0m\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstat\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m, follow_symlinks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03m    Return the result of the stat() system call on this path, like\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;124;03m    os.stat() does.\u001b[39;00m\n\u001b[1;32m   1012\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1013\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m os\u001b[38;5;241m.\u001b[39mstat(\u001b[38;5;28mself\u001b[39m, follow_symlinks\u001b[38;5;241m=\u001b[39mfollow_symlinks)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classifier.train()\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_set, batch_size=batch_size)\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=30,\n",
    "                     accelerator=\"cpu\")\n",
    "\n",
    "mlflow.pytorch.autolog()\n",
    "with mlflow.start_run() as run:\n",
    "    # mlflow.pytorch.log_model(classifier, \"model\")\n",
    "    mlflow.log_params({\n",
    "            \"batch_size\": batch_size,\n",
    "        })\n",
    "    trainer.fit(model=classifier,\n",
    "                train_dataloaders=train_loader,\n",
    "                val_dataloaders=valid_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8a168-b293-4148-915f-b6e2371ae5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "\n",
    "State = euler.Primitive_State\n",
    "left_state  = State(Density = 3.,\n",
    "                    Velocity = 0.,\n",
    "                    Pressure = 6.5)\n",
    "\n",
    "right_state = State(Density = 1.,\n",
    "                    Velocity = 0.,\n",
    "                    Pressure = 1.)\n",
    "# classifier.eval()\n",
    "# left_state  = State(Density = 1.,\n",
    "#                     Velocity = 0.,\n",
    "#                     Pressure = 1.0)\n",
    "\n",
    "# right_state = State(Density = 3.,\n",
    "#                     Velocity = 0.,\n",
    "#                     Pressure = 8.)\n",
    "\n",
    "q_l = euler.primitive_to_conservative(*left_state)\n",
    "q_r = euler.primitive_to_conservative(*right_state)\n",
    "states, speeds, reval, wave_types = euler.exact_riemann_solution(q_l, q_r, gamma=1.4)\n",
    "\n",
    "N = 256\n",
    "a = -2.0\n",
    "b = 2.0\n",
    "x = np.linspace(a, b, N)\n",
    "dx = (b - a)/N\n",
    "\n",
    "t = 0.5\n",
    "rho, u, P = euler.cons_to_prim(reval(x/t))\n",
    "classes = np.zeros(N)\n",
    "div = np.zeros(N)\n",
    "div[1:-2] = (u[2:-1] - u[0:-3])/(2 * dx)\n",
    "rho, u, P, div = StandardScaler().fit_transform(rho.reshape(-1,1)).flatten(), StandardScaler().fit_transform(u.reshape(-1,1)).flatten(), StandardScaler().fit_transform(P.reshape(-1,1)).flatten(),StandardScaler().fit_transform(div.reshape(-1,1)).flatten()\n",
    "\n",
    "\n",
    "for i in range(offset, N-offset):\n",
    "    input = np.zeros((2,stencil))\n",
    "    input[0, :] = rho[(i-offset):(i + offset + 1)]\n",
    "    #input[0, :] -= np.mean(rho[(i-offset):(i + offset + 1)])\n",
    "    # input[1, :] = u[(i-offset):(i + offset + 1)]\n",
    "    #input[1, :] -= np.mean(u[(i-offset):(i + offset + 1)])\n",
    "    input[1, :] = P[(i-offset):(i + offset + 1)]\n",
    "    #input[2, :] -= np.mean(P[(i-offset):(i + offset + 1)])\n",
    "    # input[3, :] = div[(i-offset):(i + offset + 1)]\n",
    "    #input[3, :] -= np.mean(div[(i-offset):(i + offset + 1)])\n",
    "    input = input.flatten()\n",
    "    input = torch.tensor(input, dtype=torch.float32)\n",
    "    output = classifier(input)\n",
    "    classes[i] = output.argmax()\n",
    "    if classes[i] == 1:\n",
    "        print(output)\n",
    "\n",
    "fig, axs = plt.subplots(4,1, figsize=(8,8))\n",
    "axs[0].plot(x, rho, linestyle=\"--\")\n",
    "axs[1].plot(x, u, linestyle=\"--\")\n",
    "axs[2].plot(x, P, linestyle=\"--\")\n",
    "axs[3].plot(x, div, linestyle=\"--\")\n",
    "for i in range(0, N):\n",
    "    if (classes[i] == 1):\n",
    "        axs[0].plot(x[i], rho[i], marker= 'o', markerfacecolor=\"none\", markeredgecolor=\"red\")\n",
    "        axs[1].plot(x[i], u[i], marker= 'o', markerfacecolor=\"none\", markeredgecolor=\"red\")\n",
    "        axs[2].plot(x[i], P[i], marker= 'o', markerfacecolor=\"none\", markeredgecolor=\"red\")\n",
    "    else:\n",
    "        axs[0].plot(x[i], rho[i], marker='o', markerfacecolor=\"none\", markeredgecolor=\"blue\")\n",
    "        axs[1].plot(x[i], u[i], marker= 'o', markerfacecolor=\"none\", markeredgecolor=\"blue\")\n",
    "        axs[2].plot(x[i], P[i], marker= 'o', markerfacecolor=\"none\", markeredgecolor=\"blue\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b5bb3c-3fa3-4497-80b0-826a0f8a2a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifier.state_dict(), \"../../../models/reduced/nn-red.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9beb88af-ce77-404a-843b-7c56c168daef",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.to_torchscript(\"../../../models/reduced/script-nn-red.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae344ba7-554d-4d1a-9fab-6e67bed4844a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.savetxt(\"./w-b/reduced/fc1w.txt\", classifier.state_dict()[\"fc1.weight\"].numpy())\n",
    "np.savetxt(\"./w-b/reduced/fc1b.txt\", classifier.state_dict()[\"fc1.bias\"].numpy())\n",
    "np.savetxt(\"./w-b/reduced/fc2w.txt\", classifier.state_dict()[\"fc2.weight\"].numpy())\n",
    "np.savetxt(\"./w-b/reduced/fc2b.txt\", classifier.state_dict()[\"fc2.bias\"].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2a7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ff465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
